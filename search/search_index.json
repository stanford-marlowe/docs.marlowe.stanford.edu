{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Marlowe!","text":"<p>Marlowe is a High Performance Computing (HPC) Cluster managed by Stanford Research Computing.</p> <p>It composes of an Nvidia DGX H100 Superpod, 2.5PB of DDN ExaScaler Lustre storage, and 3PB of DDN Intelliflash storage.</p> <p>With 11.1 PFlops of compute power, Marlowe would place 87th in the May 2024 top500 rankings.</p> <p>For more in-depth tech specs, refer to the Tech Specs page</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#i-receive-a-disk-quota-exceeded-error-when-writing-files-what-does-this-mean","title":"I receive a \"disk quota exceeded\" error when writing files. What does this mean?","text":"<p>Each filesystem on Marlowe has a user space quota. You're most likely to run into this issue when adding files to your home directory.</p>"},{"location":"faq/#i-cant-run-a-docker-container","title":"I can't run a docker container","text":"<p>Docker is not supported on Marlowe due to the security risks associated with it. Fortunately, Apptainer supports running docker containers natively.</p>"},{"location":"faq/#i-cant-see-my-project-directory","title":"I can't see my project directory","text":"<p>The <code>/projects/</code> filesystem uses a system called autofs to dynamically mount NFS shares in <code>/projects/</code>.</p> <p>Due to this, you may not see your specific <code>/projects/</code> directory until you first access it after login. As soon as you run a command that accesses your project directory, it will show up and be accessible. A simple way to show your <code>/projects/</code> directory is to run <code>ls /projects/&lt;project ID&gt;</code>.</p> <p>Here is an example:</p> <p></p>"},{"location":"ngc_example/","title":"Using NVIDIA NIM Containers","text":"<p>We illustrate using this Llama 3.1 (8B) model. </p> <p>NVIDIA provides Docker images for the running the model locally but Docker is ill-suited to a server infrastructure. Marlowe uses Apptainer instead and so one needs to convert the provided Docker image into an Apptainer image. Also, since the image is hosted on the NVIDIA Developer Site that requires one to authenticate,  one has to use a Docker tools on one's local machine to download the image, and then upload it to Marlowe for conversion for use with Apptainer. </p> <p>We will assume you have set up your Apptainer cache directory as noted here.</p> <ol> <li> <p>Create an NVIDIA Developer account if you haven't already done so.</p> </li> <li> <p>On your local machine, install Docker Desktop if you haven't already done so and ensure it is running.</p> </li> <li> <p>Get an API KEY for logging into NVIDIA GPU Cloud (NGC). For our example, you can obtain one by clicking on the theGet API Key at the top of the python code.</p> </li> <li> <p>In a terminal on  your local machine (<code>bluebird</code>), log into the NGC via:</p> <pre><code>bluebird$ docker login nvcr.io\n</code></pre> </li> <li> <p>Pull down the Llama image and save it as a tar file. The image is about 13G!</p> <pre><code>bluebird$ docker pull nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\nbluebird$ docker save nvcr.io/nim/meta/llama-3.1-8b-instruct:latest -o llama.tar\n</code></pre> </li> <li> <p>Upload to Marlowe project directory (assuming <code>m223813</code> is your project directory).</p> <pre><code>bluebird$ scp llama.tar login.marlowe.stanford.edu:/scratch/m223813\n</code></pre> </li> <li> <p>Log in to Marlowe and convert the image to a <code>.sif</code> file. This takes a while to complete and is done once. </p> <pre><code>you@login-02$ cd /scratch/m223813\nyou@login-02$ ml load apptainer\nyou@login-02$ apptainer build llama.sif docker-archive://llama.tar\n</code></pre> </li> <li> <p>Run an interactive queue on the partition provided for you. We use 8 GPUs in our example. Note down the node number which is typically something like <code>n01</code> or <code>n02</code> etc. We'll assume <code>n01</code> in what follows.</p> <pre><code>you@login-02$ srun --partition=&lt;your_partition&gt; --gres=gpu:8 --ntasks=1 --time=1:00:00 --pty /bin/bash\n</code></pre> </li> <li> <p>Run the container on <code>n01</code>; this will take about 10 minutes the first time. Note the use of the API Key from step 2 which can be set up once in your <code>~/.bash_profile</code> for convenience. </p> <p><pre><code>export NGC_API_KEY=&lt;your_api_key&gt; ## fill in your API key\nml load apptainer\nexport LOCAL_NIM_CACHE=$PROJDIR/.cache/nim\nmkdir -p \"$LOCAL_NIM_CACHE\"\napptainer run --nv \\\n  --bind \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n  --env NGC_API_KEY=$NGC_API_KEY \\\n  llama.sif\n</code></pre>   Once the API service has started, you will see lines like below:</p> <pre><code>INFO 2024-10-17 18:26:48.395 server.py:82] Started server process [394400]\nINFO 2024-10-17 18:26:48.395 on.py:48] Waiting for application startup.\nINFO 2024-10-17 18:26:48.409 on.py:62] Application startup complete.\nINFO 2024-10-17 18:26:48.411 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO 2024-10-17 18:27:10.522 httptools_impl.py:481] 127.0.0.1:37712 - \"POST /v1/chat/completions HTTP/1.1\" 200\n</code></pre> </li> <li> <p>Typically, one would forward the port using <code>ssh</code> to access the service on <code>localhost:8000</code> or similar, but that is not feasible in an HPC environment. At some point OnDemand applications may be set up, but until then, one can do batch processing by creating another session on the job node <code>n01</code> (log into Marlowe, and then <code>ssh n01</code>) to hit the API service. </p> </li> </ol> <p>Below is the result of a  test API call using <code>curl</code> on <code>n01</code> where we ask for a limerick about Marlowe:</p> <pre><code>curl -X 'POST' \\\n 'http://localhost:8000/v1/chat/completions' \\\n -H 'accept: application/json' \\\n -H 'Content-Type: application/json' \\\n-d '{\n   \"model\": \"meta/llama-3.1-8b-instruct\",\n   \"messages\": [{\"role\":\"user\", \"content\":\"Write a limerick about Marlowe GPU Cluster (31 DGXs)\"}],\n   \"max_tokens\": 64\n   }'\n</code></pre> <p>JSON Output:</p> <pre><code>{\"id\":\"chat-b2f3244d98b647caaa0d32ca54ed57db\",\"object\":\"chat.completion\",\"created\":1729214830,\"model\":\"meta/llama-3.1-8b-instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"There once was a cluster so fine,\\nMarlowe GPU's, with power divine,\\nThirty-one DGXs to play,\\n Made for work in a major way,\\nApplied Math's problems did align.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":28,\"total_tokens\":69,\"completion_tokens\":41}}\n</code></pre> <p>which contains:</p> <pre><code>There once was a cluster so fine,\nMarlowe GPU's, with power divine,\nThirty-one DGXs to play,\nMade for work in a major way,\nApplied Math's problems did align.\n</code></pre>"},{"location":"specs/","title":"Tech Specs","text":"<p>Marlowe consists of a DGX H100 Superpod.</p> <p>The Superpod is built out of 31 DGX H100 nodes.</p>"},{"location":"specs/#dgx-h100-specs","title":"DGX H100 Specs","text":"<p>Each DGX H100 has:</p> <ul> <li>2 x Intel Xeon Platinum 8480C</li> <li>8 x Nvidia H100 80GB</li> <li>2TB Memory</li> <li>400G NICs for fast data access</li> </ul>"},{"location":"getting-started/","title":"Getting Access","text":"<p>Once it's in production, Marlowe will require a review process to get access. The process to apply for an allocation on Marlowe is currently under development,  and will be published soon.</p> <p>Once that allocation is approved, a project will be created for you and you will be able to log in.</p>"},{"location":"getting-started/allocations/","title":"Allocations","text":"<p>To get access to Marlowe, you will need to be approved an allocation by the Marlowe Governance Group.</p>"},{"location":"getting-started/allocations/#applying-for-an-allocation","title":"Applying for an allocation","text":"<p>More info will be available soon.</p>"},{"location":"getting-started/connecting/","title":"Connecting","text":"<p>After your project has been approved, connecting is simple!</p>"},{"location":"getting-started/connecting/#ssh","title":"SSH","text":"<p>The login server you will need to ssh into is <code>login.marlowe.stanford.edu</code>. You will need to ssh in with your SUNet ID.</p> <p>So, if your SUNet ID was <code>bob</code>, your login command would look like:</p> <pre><code>ssh bob@login.marlowe.stanford.edu\n</code></pre> <p>You will be prompted to provide both your password and duo two-factor authentication credentials before it logs you in.</p>"},{"location":"getting-started/filesystems/","title":"User Filesystems","text":"<p>There are multiple filesystems available to end users on Marlowe.</p>"},{"location":"getting-started/filesystems/#users","title":"/users/","text":"<p><code>/users/</code> is where a user's home directory lives. It is directly mapped to the <code>$HOME</code> variable, so whenever you run <code>cd ~</code> or <code>cd $HOME</code> you will be sent to <code>/users/&lt;Your-sunet-ID&gt;</code>.</p> <p>For example, if my sunetID was <code>bob</code>, then my home directory on Marlowe would be <code>/users/bob</code></p> <p>Each directory in <code>/users/</code> is unique to that user.</p> <p><code>/users/</code> is backed up once every 24 hours.</p> <p>Important: Your home directory only has 15GB of storage available. For any large files or conda installs, we recommend using a different filesystem.</p>"},{"location":"getting-started/filesystems/#projects","title":"/projects/","text":"<p><code>/projects/</code> is the filesystem where all allocation project directories live.</p> <p>When you are given an allocation, you are given a project ID. This project ID is also your directory in <code>/projects/</code></p> <p>For example: If your allocation's project ID was <code>m231631</code>, then your allocation's project folder would be <code>/projects/m231631/</code></p> <p>Directory quotas in <code>/projects/</code> vary depending on the amount approved in the allocation.</p> <p>Every user added to an allocation shares the same <code>/projects/</code> folder. So <code>bob</code> and <code>amy</code> in project <code>m231631</code> would both have files in <code>/projects/m231631/</code>, but <code>greg</code> in project <code>m402630</code> would have files in <code>/projects/m402630</code></p> <p><code>/projects/</code> is backed up once every 24 hours.</p> <p>Important: Directories in <code>/projects/</code> are mounted as needed, so your specific folder may not show up when you login initially. <code>cd</code> to your project directory and it will show up until you log out.</p> <p>Here is an example:</p> <p></p>"},{"location":"getting-started/filesystems/#scratch","title":"/scratch/","text":"<p><code>/scratch/</code> is the lustre filesystem meant (as the name implies) to be used as scratch storage.</p> <p>Much like in <code>/projects/</code>, your <code>/scratch/</code> folder corresponds to your project ID. <code>/scratch/m231631</code>,<code>/scratch/m402630</code>, etc.</p> <p><code>/scratch/</code> is not backed up. Unlike <code>/users/</code> and <code>/projects/</code>, <code>/scratch/</code> is not replicated.</p>"},{"location":"getting-started/jobs/","title":"Submitting Jobs on Marlowe","text":"<p>Marlowe uses SLURM, a job scheduling system, to run jobs.</p>"},{"location":"modules/","title":"Software","text":"<p>Marlowe has multiple software packages pre-installed and available for use. </p> <p>Most are installed as modules and can be listed by running <code>module avail</code> in your terminal</p> <p>Choose a page on the left to learn more about them.</p>"},{"location":"modules/apptainer/","title":"Apptainer","text":"<p>Apptainer (formerly known as Singularity) is an HPC-oriented container system. </p> <p>Unlike Docker, Apptainer is designed for use in HPC systems such as Marlowe. More info on Apptainer can be found here: https://apptainer.org/</p> <p>To load Apptainer: <pre><code>module load apptainer\n</code></pre></p>"},{"location":"modules/apptainer/#apptainer-cache-directory","title":"Apptainer Cache Directory","text":"<p>By default, Apptainer stores all of a user's containers in their home directory.</p> <p>Since a user's home directory on Marlowe is only 15GB, that space can quickly be used up and the quota limit will block you from pulling and running containers.</p> <p>To get past this, you can set your Apptainer Cache directory to a different location. Learn more about the filesystems on Marlowe here.</p> <p>For instance, If you wanted set the apptainer cache directory to your <code>/scratch</code> directory and your project ID is <code>m223813</code>, you can run the following:</p> <pre><code>export APPTAINER_CACHEDIR=/scratch/m223813/apptainercache\n</code></pre> <p>After running the above command, all apptainer containers will be pulled to the <code>apptainercache</code> folder in your project's scratch directory.</p>"},{"location":"modules/conan/","title":"Conan","text":"<p>Conan is a C/C++ package manager.</p> <p>More info on Conan can be found here</p> <p>To load Conan, run the following:</p> <pre><code>module load conan\n</code></pre>"},{"location":"modules/conda/","title":"Conda","text":"<p>Conda is a python package manager used by a wide variety of projects.</p> <p>Marlowe uses an optimized version of Conda called Mamba.</p> <p>To load Mamba, run the following after logging in:</p> <pre><code>module load conda\n</code></pre> <p>Every conda command works with mamba. No code customization is needed.</p>"},{"location":"modules/conda/#installing-a-conda-environment-in-a-different-location","title":"Installing a Conda environment in a different location","text":"<p>Home directories have a quota of 15GB by default. This means that even small conda environments can run into quota issues rather quickly.</p> <p>To combat this, we recommend setting up your conda environment in a separate directory. You can do this with the <code>--prefix</code> command.</p> <p>For example: <pre><code>conda create --prefix /projects/m223813/mycondadir numpy=1.21\n</code></pre> The above will create a new conda environment in the <code>/projects/m223813/mycondadir</code> folder with numpy 1.21. Since it's not being installed into your home directory, you don't have to worry about the same 15GB quota as before!</p> <p>You can also use the <code>--prefix</code> argument with <code>conda env</code> and other commands.</p>"},{"location":"modules/cudnn/","title":"CuDNN","text":"<p>CuDNN is a library provided by Nvidia. More information on it can be found here: https://developer.nvidia.com/cudnn</p> <p>There are currently three versions of CuDNN available on Marlowe:</p> <pre><code>cudnn/cuda11/9.3.0.75\ncudnn/cuda12/9.3.0.75\ncudnn/cuda12/8.9.7.29\n</code></pre> <p>To load CuDNN, run the following:</p> <pre><code>module load cudnn/cuda12\n</code></pre>"},{"location":"modules/nvhpc/","title":"Nvidia HPC SDK","text":"<p>The Nvidia HPC SDK provides tools such as <code>nvcc</code> to the end user.</p> <p>Find out more here</p> <p>To load the Nvidia HPC SDK, run the following: <pre><code>module load nvhpc\n</code></pre></p>"}]}